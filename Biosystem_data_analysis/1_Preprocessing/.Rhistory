night_owlish <- "https://raw.githubusercontent.com/batpigandme/night-owlish/master/rstheme/night-owlish.rstheme"
rstudioapi::addTheme(night_owlish, apply = TRUE)
night_owlish <- "https://raw.githubusercontent.com/batpigandme/night-owlish/master/rstheme/night-owlish.rstheme"
rstudioapi::addTheme(night_owlish, apply = TRUE)
setwd("C:/Users/wangk/Desktop/MSc-Computational-Science/Biosystem_data_analysis")
setwd("C:/Users/wangk/Desktop/MSc-Computational-Science/Biosystem_data_analysis/1_Preprocessing")
counts = read.csv('counts.csv')
head(counts)
totalcounts = colSums(counts)
totalcounts
ratespermillion = as.data.frame(sweep(10^6*counts, 2, totalcounts, FUN='/'))
head(ratespermillion)
ct.names = paste('ct', 1:4, sep='.')
kd.names = paste('kd', 1:3, sep='.')
control.mean = apply(ratespermillion[ct.names], 1, mean)
control.std = apply(ratespermillion[ct.names], 1, sd)
knockdown.mean = apply(ratespermillion[kd.names], 1, mean)
knockdown.std = apply(ratespermillion[kd.names], 1, sd)
plot(control.mean, control.std, log='xy', pch=20, cex=.5, col='blue', xlab='Mean Rate', ylab='Stand Dev')
plot(knockdown.mean, knockdown.std, log='xy', pch=20, cex=.5, col='red', xlab='Mean Rate', ylab='Stand Dev')
plot(control.mean,control.sd,log = 'xy',pch = 20,cex=.5,col='blue',xlab = 'Mean Rate',ylab = 'Stand dev')
source("~/.active-rstudio-document")
totalcounts
totalcounts = colSums(counts)
source("~/.active-rstudio-document")
totalcounts
title('Standard deviation vs mean rate')
source("~/.active-rstudio-document")
all.fit(sd~mean,
data.frame(mean=log(c(control.mean, knockdown.mean)), sd=
log(c(control.sd, knockdown.sd))))
all.fit = lm(sd~mean,
data.frame(mean=log(c(control.mean, knockdown.mean)), sd=
log(c(control.sd, knockdown.sd))))
coef(all.fit)
View(ratespermillion)
t.test(ratespermillion['FBgn0000043', ct.names], ratespermillion['FBgn0000043',
kd.names], var.equal = FALSE)
pvalues.orig = apply(ratespermillion, 1, function(row){t.test(x=row[ct.names],
y=row[kd.names], var.equal=FALSE)$p.value})
transformed = as.data.frame(ratespermillion^(1-coef(all.fit)['mean']))
head(transformed)
control.t.mean = apply(ratespermillion[ct.names], 1, mean)
control.t.sd = apply(ratespermillion[ct.names], 1, sd)
knockdown.t.mean = apply(ratespermillion[kd.names], 1, mean)
knockdown.t.sd = apply(ratespermillion[kd.names], 1, sd)
plot(control.t.mean, control.t.sd, pch=20, cex=.5, col='blue',
ylim=c(-0,03, 0.3), xlab='Mean transformed value', ylab='transformed std')
plot(x=control.t.mean, y=control.t.sd, pch=20, cex=.5, col='blue',
ylim=c(-0,03, 0.3), xlab='Mean transformed value', ylab='transformed std')
plot(x=control.t.mean, y=control.t.sd, pch=20, cex=.5, col='blue',
ylim=c(-0.03, 0.3), xlab='Mean transformed value', ylab='transformed std')
point(x=knockdown.t.mean, y=knockdown.t.sd, pch=20, cex=.5, col='red')
points(x=knockdown.t.mean, y=knockdown.t.sd, pch=20, cex=.5, col='red')
title('STD versus Mean of transformed value')
control.t.mean = apply(transformed[ct.names], 1, mean)
control.t.sd = apply(transformed[ct.names], 1, sd)
knockdown.t.mean = apply(transformed[kd.names], 1, mean)
knockdown.t.sd = apply(transformed[kd.names], 1, sd)
plot(x=control.t.mean, y=control.t.sd, pch=20, cex=.5, col='blue',
ylim=c(-0.03, 0.3), xlab='Mean transformed value', ylab='transformed std')
points(x=knockdown.t.mean, y=knockdown.t.sd, pch=20, cex=.5, col='red')
title('STD versus Mean of transformed value')
# t-test on tranformed data
t.test(x=transformed['FBgn0000043', ct.names], y=transformed['FBgn0000043',
kd.names], var.equal=TRUE))
# t-test on tranformed data
t.test(x=transformed['FBgn0000043', ct.names], y=transformed['FBgn0000043',
kd.names], var.equal=TRUE)
# T-test on all rows, and extracting the p-values from the test results
pvalues.t <- apply(transformed, 1, function(row){t.test(x=row[ct.names],
y=row[kd.names], var.equal=FALSE)$p.value})
# T-test on all rows, and extracting the p-values from the test results
pvalues.t <- apply(transformed, 1, function(row){t.test(x=row[ct.names],
y=row[kd.names], var.equal=TRUE)$p.value})
# Add pvalues.t to ratespermillion table
ratespermillion$pvalue.t <- pvalue.t
# Add pvalues.t to ratespermillion table
ratespermillion$pvalue.t <- pvalue.t
# Add pvalues.t to ratespermillion table
ratespermillion$pvalue.t <- pvalues.t
head(ratespermillion)
View(ratespermillion)
# Compare the p-values before and after transformation
plot(x=-log10(pvalues.orig), y=-log10(pvalue.t), xlim=c(0.8), ylim=c(0.8),
xlab='-log10(p-values) before transformation', ylab='-log10(p-values0
after transformation')
# Compare the p-values before and after transformation
plot(x=-log10(pvalues.orig), y=-log10(pvalues.t), xlim=c(0.8), ylim=c(0.8),
xlab='-log10(p-values) before transformation', ylab='-log10(p-values0
after transformation')
# Compare the p-values before and after transformation
plot(x=-log10(pvalues.orig), y=-log10(pvalues.t), xlim=c(0,8), ylim=c(0,8),
xlab='-log10(p-values) before transformation', ylab='-log10(p-values0
after transformation')
abline(coef=(0,1), col='blue')
abline(coef=c(0,1), col='blue')
legend(0.2, 7.5, legend=c('x=y'), col=c('blue'), lty=1, box.lty=0)
title('p-values before and after transformation')
# Compare the p-values before and after transformation
plot(x=-log10(pvalues.orig), y=-log10(pvalues.t), xlim=c(0,8), ylim=c(0,8),
xlab='-log10(p-values) before transformation', ylab=
'-log10(p-values) after transformation')
abline(coef=c(0,1), col='blue')
legend(0.2, 7.5, legend=c('x=y'), col=c('blue'), lty=1, box.lty=0)
title('p-values before and after transformation')
# Calculate the log2ratio
log2ratio <- apply(counts, 1, function(x){log(sum(x[kd.names])/
sum(totalcounts[kd.names]), 2) - log(sum(x[ct.names])/
sum(totalcounts[ct.names]), 2)})
head(log2ratio)
# Combining log2ratios and p-values to create volcano plot
par(mfrow=c(1,1))
p05 = -log10(0.05)
p05 = -log10(0.05)
plot(y=-log10(pvalues.t),x=log2ratio)
lines(x=c(-1,-1),y=c(p05,7),col='gray')
lines(x=c(1,1),y=c(p05,7),col='gray')
lines(x=c(-6,-1),y=c(p05,p05),col='gray')
lines(x=c(1,4),y=c(p05,p05),col='gray')
text( x = 3, y = p05+0.2, label = "P = 0.05")
text (x=1.15, y = 6, label="FC = 2",srt=90)
title('volcano plot')
bins <- hist(pvalues.t, nclass=nbins, col='grey50', main='Histogram p-values')
# Plot a histogram of p-values
nbins <- 100
bins <- hist(pvalues.t, nclass=nbins, col='grey50', main='Histogram p-values')
# P-value cut-off
null.bins <- bins$mids > 0.4
null.level <- mean(bins$counts[null.bins])
plot(bins, col='grey50')
abline(a=null.level, b=0, col='red', lwd=2)
# Calculate False discovery rate (FDR)
fdr.05 <- null.level*nbins*0.05 / sum(pvalues.t <= 0.05)
fdr.05
# Plot the relationship between p-value cut-off and FDR
pvalue.cutoff <- seq(1/nbins, 1, by=1/nbins)
fdr <- vector(mode="numeric",length=length(pvalue.cutoff))
for (i in seq_along(pvalue.cutoff)) {
fdr[i] <- null.level*(pvalue.cutoff[i]*nbins)/sum(pvalues.t <= pvalue.cutoff[i])
}
plot(pvalue.cutoff, fdr, xlab='p-value cut-off', ylab='FDR', ylim=c(0,1))
title('FDR and p-value relationship')
setwd("C:/Users/wangk/Desktop/MSc-Computational-Science/Biosystem_data_analysis/2_PCA")
data <- read.csv('Cachexia.csv')
data <- read.csv('Cachexia.csv')
head(data[,1:3])
View(data)
# Make a plot of this data table
intensity <- as.matrix(data)
par(mfrow=c(1,2))
matplot(1:77, intensity, type='l', lwd=1, xlab='individuls', ylab='intensity')
matplot(1:63, t(intensity), type='l', lwd=1, xlab='metabolites', ylab='intensity')
# Make transformation on metabolites data
logdata <- log(intensity)
srqdata <- sqrt(intensity)
par(mfrow=c(1,3))
View(data)
hist(intensity[,1])
hist(logdata[,1])
sqrdata <- sqrt(intensity)
View(srqdata)
hist(sqrdata[,1])
# Perform PCA on both raw data and transformed data
m = colMeans(intensity)   # calculate mean value of columns
Xm = sweep(intensity, 2, m, FUN='-')   # Substract mean on each columns
ssqtotal <- sum(Xm*Xm)                 # compute sum of squares
USV <- svd(Xm)                         # Perform SVD on centered data sets
View(USV)
View(USV)
# Note that SVD() in R returns a list with length 3: d(n): single values, u(p*n):
# Unitary matrix, v(n*n): orthogonal matrix
t <- USV$u %*% diag(USV$d)
P <- USV$v                             # compute loadings (principal directions)
npc <- 10                              # compute 10 principal components
ssq <- 0 * (1:npc)                     # initialize an empty ssq vector
Xtest <- T[,i] %*% P[,i]             # calculte ssq_n / ssqtotal for each PC
for (i in 1:npc):{
Xtest <- T[,i] %*% P[,i]             # calculte ssq_n / ssqtotal for each PC
ssq[i] = 100*sum(Xtest*Xtest)/ssqtotal
}
ssq <- 0 * (1:npc)                     # initialize an empty ssq vector
for (i in 1:npc):{
Xtest <- T[,i] %*% P[,i]             # calculte ssq_n / ssqtotal for each PC
ssq[i] <- 100*sum(Xtest*Xtest)/ssqtotal
}
for (i in 1:npc):{Xtest <- T[,i] %*% P[,i]             # calculte ssq_n / ssqtotal for each PC
ssq[i] <- 100*sum(Xtest*Xtest)/ssqtotal
}
for (i in 1:npc){
Xtest <- T[,i] %*% P[,i]             # calculte ssq_n / ssqtotal for each PC
ssq[i] <- 100*sum(Xtest*Xtest)/ssqtotal
}
Xtest <- T[,i] %*% t(P[,i])             # calculte ssq_n / ssqtotal for each PC
for (i in 1:npc){
Xtest <- T[,i] %*% t(P[,i])             # calculte ssq_n / ssqtotal for each PC
ssq[i] <- 100*sum(Xtest*Xtest)/ssqtotal
}
T <- USV$u %*% diag(USV$d)             # compute scores (principal components)
P <- USV$v                             # compute loadings (principal directions)
for (i in 1:npc){
Xtest <- T[,i] %*% t(P[,i])             # calculte ssq_n / ssqtotal for each PC
ssq[i] <- 100*sum(Xtest*Xtest)/ssqtotal
}
for (i in 1:npc){
Xtest <- T[,i] %*% t(P[,i])             # calculte ssq_n / ssqtotal for each PC
ssq[i] = 100*sum(Xtest*Xtest)/ssqtotal
}
ssq[i] <- 100*sum(Xtest*Xtest)/ssqtotal
for (i in 1:npc){
Xtest <- T[,i] %*% t(P[,i])             # calculte ssq_n / ssqtotal for each PC
ssq[i] <- 100*sum(Xtest*Xtest)/ssqtotal
}
ssqcum <- cumsum(ssq)                  # calculate cumulative ssq
data.frame(ssq=ssq, ssqto=ssqcum)      # Put ssq and ssq cumulative value together
# Now do the same procedure for logdata
mlog= colMeans(logdata)
Xlogm=sweep(logdata,2,mlog,FUN="-")
ssqlogtotal <- sum(Xlogm*Xlogm)
USVlog <- svd(Xlogm)
T_log <- USVlog$u %*% diag(USVlog$d)
P_log <- USVlog$v
ssqlog <- 0 * (1:npc)
for (i in 1:npc){
Xlog_est  <- T_log[,i] %*% t(P_log[,i])
ssqlog[i] <- 100 * sum(Xlog_est*Xlog_est)/ssqlogtotal
}
ssqlogcum = cumsum(ssqlog)
data.frame(ssqlog=ssqlog,ssqlogcum=ssqlogcum)
# Now let's take a closer look at scores and residuals of both sets
par(mfrow=c(2,2))
source("~/.active-rstudio-document")
source("C:/Users/wangk/Desktop/MSc-Computational-Science/Biosystem_data_analysis/1_Preprocessing/Exercise_1.R")
